{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modelling using LSA and NMF"
      ],
      "metadata": {
        "id": "Pg4JpZYNFF0y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ItMDTFQ_06w",
        "outputId": "d2d6c60e-f741-4169-90c1-7d8a0f3db63c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = \"Budget 2023: Tax exemption removed in insurance policies with premium over Rs 5 lakh\"\n",
        "doc2 = \"Only humans can do what other humans have never done before.Creativity & ChatGPT - The only engineering course\"\n",
        "doc3 = \"ChatGPT maker OpenAI releases tool to identify AI-written text.It can mislabel both AI-generated and human-written text, and it can also be evaded with minor edits.OpenAI's AI Text Classifier can help to detect AI-generated content, but it is not 100% accurate and can make mistakes.\"\n",
        "doc4 = \"In Budget Health allocation puts focus on pharma research, collaborative R&D at ICMR labs. Microsoft Research Proposes BioGPT: A Domain-Specific Generative Transformer Language Model Pre-Trained on Large-Scale Biomedical Literature.With recent technological breakthroughs, researchers have started employing several machine learning techniques on the abundance of biomedical data that is available. Using techniques like text mining and knowledge extraction on biomedical literature has been demonstrated to be crucial in developing new medications, clinical therapy, pathology research, etc.\"\n",
        "doc5 = \"The Kannada Movie was really good. The reviews for the moview were quite good.The 2023 Budget was balanced and prudnet with no negatives and surprises.\"\n",
        "doc6 = \"This was one of the best books I had read. The book was the best seller.  Most of the movies released on Netflix and Amazon are good to watch.\""
      ],
      "metadata": {
        "id": "LMTM_wxF_8sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [doc1, doc2, doc3, doc4, doc5, doc6]"
      ],
      "metadata": {
        "id": "54o9nI-g__CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data = []\n",
        "\n",
        "lemma =WordNetLemmatizer()\n",
        "stopwords =stopwords.words(\"english\")\n",
        "for text in corpus:\n",
        "  text = re.sub(r\"https\\S+\", \"\", text) # removing links\n",
        "  text = re.sub(\"[^a-zA-Z0-9]\", \" \", text) # including only alphabets and numericals\n",
        "  text = nltk.word_tokenize(text.lower()) # tokenization\n",
        "  text = [lemma.lemmatize(word) for word in text] # lemmatization\n",
        "  text = [word for word in text if word not in stopwords] # stopwords\n",
        "  text = \" \".join(text)\n",
        "  cleaned_data.append(text)"
      ],
      "metadata": {
        "id": "zh2QEjIAABhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zDODqBWADby",
        "outputId": "9e70d73b-3aab-441d-e9da-ed3849a048c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['budget 2023 tax exemption removed insurance policy premium r 5 lakh',\n",
              " 'human human never done creativity chatgpt engineering course',\n",
              " 'chatgpt maker openai release tool identify ai written text mislabel ai generated human written text also evaded minor edits openai ai text classifier help detect ai generated content 100 accurate make mistake',\n",
              " 'budget health allocation put focus pharma research collaborative r icmr lab microsoft research proposes biogpt domain specific generative transformer language model pre trained large scale biomedical literature recent technological breakthrough researcher started employing several machine learning technique abundance biomedical data available using technique like text mining knowledge extraction biomedical literature ha demonstrated crucial developing new medication clinical therapy pathology research etc',\n",
              " 'kannada movie wa really good review moview quite good 2023 budget wa balanced prudnet negative surprise',\n",
              " 'wa one best book read book wa best seller movie released netflix amazon good watch']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_features=50)\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(cleaned_data)\n",
        "tfidf_vectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khke3ElpAREj",
        "outputId": "1488316b-ea02-428e-89fe-d16f109aac24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<6x50 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 59 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
        "vocab_tfidf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl-2wirQEvaC",
        "outputId": "c1a4d541-6d77-48e3-e3e7-e1fe239d366b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['2023', 'ai', 'best', 'biomedical', 'book', 'budget', 'chatgpt',\n",
              "       'generated', 'good', 'human', 'literature', 'minor', 'mislabel',\n",
              "       'mistake', 'model', 'movie', 'moview', 'negative', 'netflix',\n",
              "       'never', 'new', 'one', 'openai', 'pathology', 'pharma', 'policy',\n",
              "       'pre', 'premium', 'proposes', 'prudnet', 'quite', 'release',\n",
              "       'research', 'scale', 'seller', 'several', 'specific', 'started',\n",
              "       'surprise', 'tax', 'technique', 'technological', 'text', 'therapy',\n",
              "       'tool', 'trained', 'transformer', 'using', 'wa', 'written'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_tfidf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzcVCaMCEyNi",
        "outputId": "5e4fac73-ddc8-426a-e63f-72df248d9f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50,)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "lsa_model = TruncatedSVD(n_components=3,n_iter = 10)\n",
        "lsa_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pvTrd1bAULY",
        "outputId": "80e96b65-4ab7-47ac-f9df-d6bc521a6105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TruncatedSVD(n_components=3, n_iter=10)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lsa_topics = lsa_model.fit_transform(tfidf_vectors)\n",
        "lsa_topics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46UdtdDVA4KA",
        "outputId": "73f1fa78-f857-44d8-acf5-d2c15c3d854e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.35653114,  0.03564047,  0.64059766],\n",
              "       [ 0.00478586,  0.722693  , -0.22474923],\n",
              "       [ 0.01212778,  0.76003658, -0.04494174],\n",
              "       [ 0.0806577 ,  0.23816777,  0.67071025],\n",
              "       [ 0.8304777 , -0.01812772, -0.02987864],\n",
              "       [ 0.74940539, -0.03941603, -0.3416799 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Document 1\")\n",
        "for i ,topic in enumerate(lsa_topics[0]):\n",
        "  print(\"Topic: \", i, \"-\", topic*100, \"%\")\n",
        "print(\"\\nDocument 2\")\n",
        "for i ,topic in enumerate(lsa_topics[1]):\n",
        "  print(\"Topic: \", i, \"-\", topic*100, \"%\")\n",
        "print(\"\\nDocument 3\")\n",
        "for i ,topic in enumerate(lsa_topics[2]):\n",
        "  print(\"Topic: \", i, \"-\", topic*100, \"%\")\n",
        "print(\"\\nDocument 4\")\n",
        "for i ,topic in enumerate(lsa_topics[3]):\n",
        "  print(\"Topic: \", i, \"-\", topic*100, \"%\")\n",
        "print(\"\\nDocument 5\")\n",
        "for i ,topic in enumerate(lsa_topics[4]):\n",
        "  print(\"Topic: \", i, \"-\", topic*100, \"%\")\n",
        "print(\"\\nDocument 6\")\n",
        "for i ,topic in enumerate(lsa_topics[5]):\n",
        "  print(\"Topic: \", i, \"-\", topic*100, \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oygf_QbFBDvS",
        "outputId": "d748417d-bfc6-4f34-912f-b5caaa5c6dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1\n",
            "Topic:  0 - 35.65311405818915 %\n",
            "Topic:  1 - 3.5640465490021653 %\n",
            "Topic:  2 - 64.0597662372234 %\n",
            "\n",
            "Document 2\n",
            "Topic:  0 - 0.47858564929111647 %\n",
            "Topic:  1 - 72.26930027876217 %\n",
            "Topic:  2 - -22.47492261538427 %\n",
            "\n",
            "Document 3\n",
            "Topic:  0 - 1.2127776004955098 %\n",
            "Topic:  1 - 76.0036579026904 %\n",
            "Topic:  2 - -4.494174147136258 %\n",
            "\n",
            "Document 4\n",
            "Topic:  0 - 8.0657696932792 %\n",
            "Topic:  1 - 23.81677690333808 %\n",
            "Topic:  2 - 67.07102480865753 %\n",
            "\n",
            "Document 5\n",
            "Topic:  0 - 83.04776973399164 %\n",
            "Topic:  1 - -1.812772231334458 %\n",
            "Topic:  2 - -2.9878637094128564 %\n",
            "\n",
            "Document 6\n",
            "Topic:  0 - 74.9405392119326 %\n",
            "Topic:  1 - -3.9416034575524486 %\n",
            "Topic:  2 - -34.16798960636378 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lsa_model.components_.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxaVjEieBp2Q",
        "outputId": "e99f7728-662d-4f49-9c01-0e64ef59f88b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lsa_model.components_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cL8NdpvC4bD",
        "outputId": "9d8829af-658d-4c5c-97e3-d2fda2c4e405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.24433545,  0.00551049,  0.27907645,  0.02690797,  0.27907645,\n",
              "         0.21249435,  0.00248628,  0.00275524,  0.39591018,  0.00384288,\n",
              "         0.01793864,  0.00137762,  0.00137762,  0.00137762,  0.00896932,\n",
              "         0.25516676,  0.17163532,  0.17163532,  0.13953822,  0.00165437,\n",
              "         0.00896932,  0.13953822,  0.00275524,  0.00896932,  0.00896932,\n",
              "         0.12632954,  0.00896932,  0.12632954,  0.00896932,  0.17163532,\n",
              "         0.17163532,  0.00137762,  0.02690797,  0.00896932,  0.13953822,\n",
              "         0.00896932,  0.00896932,  0.00896932,  0.17163532,  0.12632954,\n",
              "         0.01793864,  0.00896932,  0.01074398,  0.00896932,  0.00137762,\n",
              "         0.00896932,  0.00896932,  0.00896932,  0.51033352,  0.00275524],\n",
              "       [ 0.00869804,  0.4124129 , -0.01752945,  0.09488708, -0.01752945,\n",
              "         0.02924065,  0.32919154,  0.20620645, -0.01452494,  0.57383695,\n",
              "         0.06325805,  0.10310323,  0.10310323,  0.10310323,  0.03162903,\n",
              "        -0.01085607, -0.00447415, -0.00447415, -0.00876473,  0.29834286,\n",
              "         0.03162903, -0.00876473,  0.20620645,  0.03162903,  0.03162903,\n",
              "         0.01508134,  0.03162903,  0.01508134,  0.03162903, -0.00447415,\n",
              "        -0.00447415,  0.10310323,  0.09488708,  0.03162903, -0.00876473,\n",
              "         0.03162903,  0.03162903,  0.03162903, -0.00447415,  0.01508134,\n",
              "         0.06325805,  0.03162903,  0.27957461,  0.03162903,  0.10310323,\n",
              "         0.03162903,  0.03162903,  0.03162903, -0.02171214,  0.20620645],\n",
              "       [ 0.24339608, -0.02744964, -0.1710424 ,  0.30077933, -0.1710424 ,\n",
              "         0.27490278, -0.09126604, -0.01372482, -0.08374208, -0.1769048 ,\n",
              "         0.20051955, -0.00686241, -0.00686241, -0.00686241,  0.10025978,\n",
              "        -0.07693534, -0.00830075, -0.00830075, -0.0855212 , -0.1044357 ,\n",
              "         0.10025978, -0.0855212 , -0.01372482,  0.10025978,  0.10025978,\n",
              "         0.30512006,  0.10025978,  0.30512006,  0.10025978, -0.00830075,\n",
              "        -0.00830075, -0.00686241,  0.30077933,  0.10025978, -0.0855212 ,\n",
              "         0.10025978,  0.10025978,  0.10025978, -0.00830075,  0.30512006,\n",
              "         0.20051955,  0.10025978,  0.06533263,  0.10025978, -0.00686241,\n",
              "         0.10025978,  0.10025978,  0.10025978, -0.15387068, -0.01372482]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_words_in_each_topic = 6\n",
        "for i,  topic_dist in enumerate(lsa_model.components_):\n",
        "  sorted_topic_dist = np.argsort(topic_dist)\n",
        "  topic_words = np.array(vocab_tfidf)[sorted_topic_dist]\n",
        "  #topic_words = topic_words[:-n_words_in_each_topic:-1]\n",
        "  print(topic_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZj6TcqSC5Nd",
        "outputId": "b633a7d5-a4a6-4b37-a960-7c88ff4be23d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['minor' 'release' 'mistake' 'mislabel' 'tool' 'never' 'chatgpt' 'written'\n",
            " 'generated' 'openai' 'human' 'ai' 'therapy' 'technological' 'pre'\n",
            " 'trained' 'transformer' 'started' 'specific' 'several' 'scale' 'using'\n",
            " 'proposes' 'pharma' 'new' 'model' 'pathology' 'text' 'technique'\n",
            " 'literature' 'research' 'biomedical' 'policy' 'premium' 'tax' 'seller'\n",
            " 'netflix' 'one' 'quite' 'surprise' 'prudnet' 'moview' 'negative' 'budget'\n",
            " '2023' 'movie' 'best' 'book' 'good' 'wa']\n",
            "['wa' 'book' 'best' 'good' 'movie' 'one' 'seller' 'netflix' 'prudnet'\n",
            " 'quite' 'surprise' 'negative' 'moview' '2023' 'policy' 'premium' 'tax'\n",
            " 'budget' 'proposes' 'started' 'technological' 'scale' 'therapy' 'trained'\n",
            " 'transformer' 'using' 'several' 'specific' 'pharma' 'pathology' 'pre'\n",
            " 'model' 'new' 'technique' 'literature' 'research' 'biomedical' 'mistake'\n",
            " 'mislabel' 'minor' 'release' 'tool' 'openai' 'generated' 'written' 'text'\n",
            " 'never' 'chatgpt' 'ai' 'human']\n",
            "['human' 'best' 'book' 'wa' 'never' 'chatgpt' 'seller' 'one' 'netflix'\n",
            " 'good' 'movie' 'ai' 'openai' 'written' 'generated' 'surprise' 'moview'\n",
            " 'quite' 'negative' 'prudnet' 'release' 'mistake' 'tool' 'minor'\n",
            " 'mislabel' 'text' 'therapy' 'using' 'technological' 'trained' 'started'\n",
            " 'specific' 'several' 'transformer' 'pharma' 'proposes' 'pre' 'pathology'\n",
            " 'new' 'model' 'scale' 'technique' 'literature' '2023' 'budget' 'research'\n",
            " 'biomedical' 'premium' 'tax' 'policy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_to_topics = lsa_model.transform(tfidf_vectors)\n",
        "for n in range(doc_to_topics.shape[0]):\n",
        "  topic_doc = doc_to_topics[n].argmax()\n",
        "  print(\"Document:\" , n+1, \"--> Topic\", topic_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4l5iGawFvUw",
        "outputId": "0439698a-3866-4bc4-d433-4875687002cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document: 1 --> Topic 2\n",
            "Document: 2 --> Topic 1\n",
            "Document: 3 --> Topic 1\n",
            "Document: 4 --> Topic 2\n",
            "Document: 5 --> Topic 0\n",
            "Document: 6 --> Topic 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Non-Negative Matrix Factorization"
      ],
      "metadata": {
        "id": "uj47499yHvNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import NMF"
      ],
      "metadata": {
        "id": "k3XtLEkfHujU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nmf_model =NMF(n_components=3)\n",
        "nmf_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s1X0j-6Gp9P",
        "outputId": "6b465e45-07dc-4692-b06e-9844760c2616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NMF(n_components=3)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nmf_topics = nmf_model.fit_transform(tfidf_vectors)\n",
        "nmf_topics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-FHxtAqIM4x",
        "outputId": "354f8ca7-9326-4f06-949c-8946ed7a9388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.89146547e-002, 0.00000000e+000, 7.64162921e-001],\n",
              "       [3.12344427e-103, 7.23390848e-001, 0.00000000e+000],\n",
              "       [0.00000000e+000, 7.21872960e-001, 1.79077870e-002],\n",
              "       [0.00000000e+000, 1.55321286e-002, 7.37613604e-001],\n",
              "       [7.30416901e-001, 0.00000000e+000, 8.16698977e-002],\n",
              "       [7.50637503e-001, 0.00000000e+000, 0.00000000e+000]])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Document 1\")\n",
        "for i ,topic in enumerate(nmf_topics[0]):\n",
        "  print(\"Topic: \", i, \"-\", topic*100, \"%\")\n",
        "print(\"\\nDocument 2\")\n",
        "for i ,topic in enumerate(nmf_topics[1]):\n",
        "  print(\"Topic: \", i, \"-\", topic*100, \"%\")\n",
        "print(\"\\nDocument 3\")\n",
        "for i ,topic in enumerate(nmf_topics[2]):\n",
        "  print(\"Topic: \", i, \"-\", topic*100, \"%\")\n",
        "print(\"\\nDocument 4\")\n",
        "for i ,topic in enumerate(nmf_topics[3]):\n",
        "  print(\"Topic: \", i, \"-\", topic*100, \"%\")\n",
        "print(\"\\nDocument 5\")\n",
        "for i ,topic in enumerate(nmf_topics[4]):\n",
        "  print(\"Topic: \", i, \"-\", topic*100, \"%\")\n",
        "print(\"\\nDocument 6\")\n",
        "for i ,topic in enumerate(nmf_topics[5]):\n",
        "  print(\"Topic: \", i, \"-\", topic*100, \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEsOI4dzIVJH",
        "outputId": "a026122d-fe24-4bfc-ecc5-2b20f5e98add"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1\n",
            "Topic:  0 - 2.8914654691891775 %\n",
            "Topic:  1 - 0.0 %\n",
            "Topic:  2 - 76.41629206429705 %\n",
            "\n",
            "Document 2\n",
            "Topic:  0 - 3.123444268123779e-101 %\n",
            "Topic:  1 - 72.33908475724385 %\n",
            "Topic:  2 - 0.0 %\n",
            "\n",
            "Document 3\n",
            "Topic:  0 - 0.0 %\n",
            "Topic:  1 - 72.18729595000072 %\n",
            "Topic:  2 - 1.7907787012075504 %\n",
            "\n",
            "Document 4\n",
            "Topic:  0 - 0.0 %\n",
            "Topic:  1 - 1.5532128569415156 %\n",
            "Topic:  2 - 73.7613603856882 %\n",
            "\n",
            "Document 5\n",
            "Topic:  0 - 73.04169014688738 %\n",
            "Topic:  1 - 0.0 %\n",
            "Topic:  2 - 8.16698976564774 %\n",
            "\n",
            "Document 6\n",
            "Topic:  0 - 75.06375033269497 %\n",
            "Topic:  1 - 0.0 %\n",
            "Topic:  2 - 0.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# n_words_in_each_topic = 6\n",
        "for i,  topic_dist in enumerate(nmf_model.components_):\n",
        "  sorted_topic_dist = np.argsort(topic_dist)\n",
        "  topic_words = np.array(vocab_tfidf)[sorted_topic_dist]\n",
        "  #topic_words = topic_words[:-n_words_in_each_topic:-1]\n",
        "  print(topic_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfBhyJBZI8en",
        "outputId": "a1e3fa25-a61d-4b16-ff23-07970b1fd35c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['pharma' 'policy' 'pre' 'premium' 'proposes' 'release' 'research' 'scale'\n",
            " 'several' 'specific' 'started' 'tax' 'technique' 'technological' 'text'\n",
            " 'therapy' 'tool' 'trained' 'transformer' 'using' 'pathology' 'openai'\n",
            " 'written' 'new' 'ai' 'biomedical' 'generated' 'literature' 'mislabel'\n",
            " 'mistake' 'model' 'minor' 'chatgpt' 'never' 'human' 'budget' '2023'\n",
            " 'seller' 'one' 'netflix' 'quite' 'surprise' 'negative' 'moview' 'prudnet'\n",
            " 'movie' 'book' 'best' 'good' 'wa']\n",
            "['2023' 'policy' 'pre' 'premium' 'proposes' 'prudnet' 'quite' 'research'\n",
            " 'scale' 'seller' 'wa' 'several' 'started' 'surprise' 'tax' 'technique'\n",
            " 'technological' 'therapy' 'trained' 'transformer' 'using' 'specific'\n",
            " 'pathology' 'pharma' 'one' 'best' 'biomedical' 'book' 'budget' 'good'\n",
            " 'model' 'literature' 'movie' 'new' 'moview' 'netflix' 'negative'\n",
            " 'mistake' 'mislabel' 'minor' 'tool' 'release' 'written' 'generated'\n",
            " 'openai' 'text' 'never' 'chatgpt' 'ai' 'human']\n",
            "['seller' 'best' 'wa' 'book' 'chatgpt' 'one' 'never' 'human' 'netflix'\n",
            " 'movie' 'release' 'minor' 'mislabel' 'mistake' 'tool' 'openai' 'written'\n",
            " 'generated' 'ai' 'good' 'negative' 'quite' 'prudnet' 'surprise' 'moview'\n",
            " 'text' 'several' 'started' 'specific' 'transformer' 'therapy' 'trained'\n",
            " 'using' 'technological' 'pharma' 'proposes' 'pre' 'pathology' 'new'\n",
            " 'model' 'scale' 'technique' 'literature' '2023' 'research' 'biomedical'\n",
            " 'budget' 'premium' 'tax' 'policy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_to_topics = nmf_model.transform(tfidf_vectors)\n",
        "for n in range(doc_to_topics.shape[0]):\n",
        "  topic_doc = doc_to_topics[n].argmax()\n",
        "  print(\"Document:\" , n+1, \"--> Topic\", topic_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBJRjB7iJEaB",
        "outputId": "de4296de-e1d1-4893-c398-f24b8f08bc13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document: 1 --> Topic 2\n",
            "Document: 2 --> Topic 1\n",
            "Document: 3 --> Topic 1\n",
            "Document: 4 --> Topic 2\n",
            "Document: 5 --> Topic 0\n",
            "Document: 6 --> Topic 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The topic which has the highest probability is the topic of that document.The Topics assigned to the documents are different from LDA and the LSA model.LSA is based on the principal componenets and the LDA is a probabilistic approach. The  results obtained from both LSA and NMF are the same."
      ],
      "metadata": {
        "id": "3ilPTf3qbJlI"
      }
    }
  ]
}