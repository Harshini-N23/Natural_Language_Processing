{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Topic Modelling using LDA, LSA, and NMF\n",
        "## Aim: To identify the  topics that best describes a set of documents.\n",
        "## Dataset: Newspaper Articles\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ndyrBGnWYjoy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBndTqekAsrj",
        "outputId": "2fcf8ab1-231a-4a5b-948f-3576e131c136"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = \"Budget 2023: Tax exemption removed in insurance policies with premium over Rs 5 lakh\"\n",
        "doc2 = \"Only humans can do what other humans have never done before.Creativity & ChatGPT - The only engineering course\"\n",
        "doc3 = \"ChatGPT maker OpenAI releases tool to identify AI-written text.It can mislabel both AI-generated and human-written text, and it can also be evaded with minor edits.OpenAI's AI Text Classifier can help to detect AI-generated content, but it is not 100% accurate and can make mistakes.\"\n",
        "doc4 = \"In Budget Health allocation puts focus on pharma research, collaborative R&D at ICMR labs. Microsoft Research Proposes BioGPT: A Domain-Specific Generative Transformer Language Model Pre-Trained on Large-Scale Biomedical Literature.With recent technological breakthroughs, researchers have started employing several machine learning techniques on the abundance of biomedical data that is available. Using techniques like text mining and knowledge extraction on biomedical literature has been demonstrated to be crucial in developing new medications, clinical therapy, pathology research, etc.\"\n",
        "doc5 = \"The Kannada Movie was really good. The reviews for the moview were quite good.The 2023 Budget was balanced and prudnet with no negatives and surprises.\"\n",
        "doc6 = \"This was one of the best books I had read. The book was the best seller.  Most of the movies released on Netflix and Amazon are good to watch.\""
      ],
      "metadata": {
        "id": "0i5lbzI3BBzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Budget\n",
        "movie\n",
        "technology"
      ],
      "metadata": {
        "id": "FHVESy-HFw07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [doc1, doc2, doc3, doc4, doc5, doc6]"
      ],
      "metadata": {
        "id": "XWbQfl4wGga9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data = []\n",
        "\n",
        "lemma =WordNetLemmatizer()\n",
        "stopwords =stopwords.words(\"english\")\n",
        "for text in corpus:\n",
        "  text = re.sub(r\"https\\S+\", \"\", text) # removing links\n",
        "  text = re.sub(\"[^a-zA-Z0-9]\", \" \", text) # including only alphabets and numericals\n",
        "  text = nltk.word_tokenize(text.lower()) # tokenization\n",
        "  text = [lemma.lemmatize(word) for word in text] # lemmatization\n",
        "  text = [word for word in text if word not in stopwords] # stopwords\n",
        "  text = \" \".join(text)\n",
        "  cleaned_data.append(text)"
      ],
      "metadata": {
        "id": "9uFG-oNOGk61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_TRiF1oHmib",
        "outputId": "88747ada-b35d-4841-f507-735104026d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['budget 2023 tax exemption removed insurance policy premium r 5 lakh',\n",
              " 'human human never done creativity chatgpt engineering course',\n",
              " 'chatgpt maker openai release tool identify ai written text mislabel ai generated human written text also evaded minor edits openai ai text classifier help detect ai generated content 100 accurate make mistake',\n",
              " 'budget health allocation put focus pharma research collaborative r icmr lab microsoft research proposes biogpt domain specific generative transformer language model pre trained large scale biomedical literature recent technological breakthrough researcher started employing several machine learning technique abundance biomedical data available using technique like text mining knowledge extraction biomedical literature ha demonstrated crucial developing new medication clinical therapy pathology research etc',\n",
              " 'kannada movie wa really good review moview quite good 2023 budget wa balanced prudnet negative surprise',\n",
              " 'wa one best book read book wa best seller movie released netflix amazon good watch']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frequent_words =nltk.FreqDist(word for text in cleaned_data for word in text.lower().split())\n",
        "frequent_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNzDc4X3ISVB",
        "outputId": "c3c2f8b6-7cfc-48cf-8aea-e2c03d225ac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'ai': 4, 'text': 4, 'wa': 4, 'budget': 3, 'human': 3, 'research': 3, 'biomedical': 3, 'good': 3, '2023': 2, 'r': 2, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing LDA using Sklearn"
      ],
      "metadata": {
        "id": "sJapTAvwJIzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_features=50)\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(cleaned_data)"
      ],
      "metadata": {
        "id": "ceuieYnqIklw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
        "vocab_tfidf"
      ],
      "metadata": {
        "id": "VHi3kRzhJlNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = LatentDirichletAllocation(n_components = 3, max_iter = 20)\n",
        "topics1 = lda_model.fit_transform(tfidf_vectors)\n",
        "topic1_words = lda_model.components_\n",
        "topic1_words"
      ],
      "metadata": {
        "id": "QxBCC0EwJ0mj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dd50127-1e9d-4129-cfe7-945504ad8ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5649975 , 0.96050474, 0.33451166, 0.33406526, 0.33451166,\n",
              "        0.52628774, 0.45731556, 0.64573686, 0.80773644, 0.44796189,\n",
              "        0.33407779, 0.48853981, 0.48853981, 0.48853981, 0.33402993,\n",
              "        0.56764122, 0.6173765 , 0.6173765 , 0.33453815, 0.33492572,\n",
              "        0.33402993, 0.33453815, 0.64573686, 0.33402993, 0.33402993,\n",
              "        0.33465832, 0.33402993, 0.33465832, 0.33402993, 0.6173765 ,\n",
              "        0.6173765 , 0.48853981, 0.33406526, 0.33402993, 0.33453815,\n",
              "        0.33402993, 0.33402993, 0.33402993, 0.6173765 , 0.33465832,\n",
              "        0.33407779, 0.33402993, 0.72134068, 0.33402993, 0.48853981,\n",
              "        0.33402993, 0.33402993, 0.33402993, 0.80332501, 0.64573686],\n",
              "       [0.73635882, 0.33424455, 0.33426657, 0.79391318, 0.33426657,\n",
              "        0.7826417 , 0.33566313, 0.33428495, 0.33527925, 0.33555508,\n",
              "        0.63986215, 0.33422689, 0.33422689, 0.33422689, 0.48593461,\n",
              "        0.3352933 , 0.33438336, 0.33438336, 0.3342875 , 0.33483383,\n",
              "        0.48593461, 0.3342875 , 0.33428495, 0.48593461, 0.48593461,\n",
              "        0.82144519, 0.48593461, 0.82144519, 0.48593461, 0.33438336,\n",
              "        0.33438336, 0.33422689, 0.79391318, 0.48593461, 0.3342875 ,\n",
              "        0.48593461, 0.48593461, 0.48593461, 0.33438336, 0.82144519,\n",
              "        0.63986215, 0.48593461, 0.45669511, 0.48593461, 0.33422689,\n",
              "        0.48593461, 0.48593461, 0.48593461, 0.33528845, 0.33428495],\n",
              "       [0.33582493, 0.33459352, 0.84702602, 0.33409777, 0.84702602,\n",
              "        0.33565645, 0.72865914, 0.3346496 , 0.53793863, 1.13074118,\n",
              "        0.33411086, 0.33456901, 0.33456901, 0.33456901, 0.33406086,\n",
              "        0.54328435, 0.33449789, 0.33449789, 0.58907647, 0.80903735,\n",
              "        0.33406086, 0.58907647, 0.3346496 , 0.33406086, 0.33406086,\n",
              "        0.33467549, 0.33406086, 0.33467549, 0.33406086, 0.33449789,\n",
              "        0.33449789, 0.33456901, 0.33409777, 0.33406086, 0.58907647,\n",
              "        0.33406086, 0.33406086, 0.33406086, 0.33449789, 0.33467549,\n",
              "        0.33411086, 0.33406086, 0.33531982, 0.33406086, 0.33456901,\n",
              "        0.33406086, 0.33406086, 0.33406086, 0.75382429, 0.3346496 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i ,topic in enumerate(topics1[0]):\n",
        "  print(\"Topic: \", i, \"-\", topic*100, \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBwlKA8mYWFq",
        "outputId": "068a1966-18e1-45cb-82e8-39704d2268c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic:  0 - 10.535887966614272 %\n",
            "Topic:  1 - 78.73557778656493 %\n",
            "Topic:  2 - 10.728534246820807 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_words_in_each_topic = 6\n",
        "for i,  topic_dist in enumerate(topic1_words):\n",
        "  sorted_topic_dist = np.argsort(topic_dist)\n",
        "  topic_words = np.array(vocab_tfidf)[sorted_topic_dist]\n",
        "  topic_words = topic_words[:-n_words_in_each_topic:-1]\n",
        "  print(topic_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVwd2y9WK-E5",
        "outputId": "b96fb2a6-0865-4778-9c9b-608f8974cc60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ai' 'good' 'wa' 'text' 'written']\n",
            "['policy' 'tax' 'premium' 'biomedical' 'research']\n",
            "['human' 'best' 'book' 'never' 'wa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_to_topics = lda_model.transform(tfidf_vectors)\n",
        "for n in range(doc_to_topics.shape[0]):\n",
        "  topic_doc = doc_to_topics[n].argmax()\n",
        "  print(\"Document:\" , n+1, \"--> Topic\", topic_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tjp8GjZ7Lgfj",
        "outputId": "1ecbcc59-1df2-4ff2-8dd7-32cdc5cb7499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document: 1 --> Topic 1\n",
            "Document: 2 --> Topic 2\n",
            "Document: 3 --> Topic 0\n",
            "Document: 4 --> Topic 1\n",
            "Document: 5 --> Topic 0\n",
            "Document: 6 --> Topic 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion:\n",
        "###  The given corpus consists of news-articles belonging to Economy, Technology and Entertainment. The Topic 0 as modelled by the algorithm consists of technology related words, Topic 1 relates to Economy and Topic 2 relates to mixture of both. Thus, each of the 6 documents have been assigned to any of the three topics.  The document 1 is related to economy and the topic assigned to it is also economy.\n"
      ],
      "metadata": {
        "id": "qyPQMMID7yKP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A7xMQ3yx7x1i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}